{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuqDQr86qMN4",
        "outputId": "37df3ff8-9747-4e1c-ec25-ddd10c40e2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 12s 388ms/step - loss: 0.6492 - accuracy: 0.6961 - val_loss: 0.6022 - val_accuracy: 0.7059\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 6s 453ms/step - loss: 0.5617 - accuracy: 0.7255 - val_loss: 0.5274 - val_accuracy: 0.7059\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 5s 352ms/step - loss: 0.4202 - accuracy: 0.7868 - val_loss: 0.4823 - val_accuracy: 0.7451\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 5s 366ms/step - loss: 0.2441 - accuracy: 0.9436 - val_loss: 0.5284 - val_accuracy: 0.7843\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 5s 402ms/step - loss: 0.1266 - accuracy: 0.9730 - val_loss: 0.7046 - val_accuracy: 0.7941\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 4s 338ms/step - loss: 0.1108 - accuracy: 0.9828 - val_loss: 0.7112 - val_accuracy: 0.7549\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 6s 452ms/step - loss: 0.0747 - accuracy: 0.9828 - val_loss: 0.8155 - val_accuracy: 0.7647\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 4s 340ms/step - loss: 0.0911 - accuracy: 0.9853 - val_loss: 0.7769 - val_accuracy: 0.8235\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 5s 349ms/step - loss: 0.0533 - accuracy: 0.9951 - val_loss: 0.7810 - val_accuracy: 0.8039\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 6s 457ms/step - loss: 0.0337 - accuracy: 0.9951 - val_loss: 0.8399 - val_accuracy: 0.8235\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.8192 - accuracy: 0.8203\n",
            "Test Loss: 0.8192, Test Accuracy: 0.8203\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "# Load the pilot dataset from Excel\n",
        "pilot_dataset_file = \"hand_coded_dataset.xlsx\"\n",
        "pilot_df = pd.read_excel(pilot_dataset_file)\n",
        "\n",
        "# Shuffle the data to avoid any bias in the ordering\n",
        "pilot_df = pilot_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split data into features and labels\n",
        "X_text_features = pilot_df[[\"subject\", \"sender\", \"sender_email\", \"body\", \"phishing\"]]\n",
        "y = pilot_df[\"persuation\"].values\n",
        "\n",
        "# Combine text features into a single column\n",
        "X = X_text_features.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Tokenize the text data\n",
        "max_words = 10000  # Maximum number of words to keep in the vocabulary\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Convert text data to sequences of integers\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "max_sequence_length = 100  # Maximum length of sequences\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding=\"post\")\n",
        "\n",
        "# Define the BiLSTM model\n",
        "embedding_dim = 100  # Dimensionality of word embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Split data into training and test sets\n",
        "test_size = 0.2\n",
        "split_idx = int(len(X_padded) * (1 - test_size))\n",
        "X_train, X_test = X_padded[:split_idx], X_padded[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6gM-CQ_y0d2",
        "outputId": "9dd09109-c83b-46f0-9339-6483c1a5b795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1042/1042 [==============================] - 50s 48ms/step\n",
            "                                                 subject  \\\n",
            "0                                 Re: Credit Derivatives   \n",
            "1                              Meter #1591 Lamay Gaslift   \n",
            "2                                   Re: man night again?   \n",
            "3                                Enron 480, 1480 charges   \n",
            "4                                         Transport Deal   \n",
            "...                                                  ...   \n",
            "33325                        Online Banking Notification   \n",
            "33326  HSBC Internet Banking : Temporary Access Suspe...   \n",
            "33327   Please Read: Important Message From Halifax Bank   \n",
            "33328                  HSBC Mail - Alert Account Locked!   \n",
            "33329                           CUSTOMER SERVICE MESSAGE   \n",
            "\n",
            "                                    sender               sender_email  \\\n",
            "0                          Sara Shackleton  sara.shackleton@enron.com   \n",
            "1                               Pat Clynes       pat.clynes@enron.com   \n",
            "2              chad knipe <knipe3@msn.com>             knipe3@msn.com   \n",
            "3      Almeida, Keoni <KAlmeida@caiso.com>         kalmeida@caiso.com   \n",
            "4                            Chris Germany    chris.germany@enron.com   \n",
            "...                                    ...                        ...   \n",
            "33325                  Hsbc Online Banking                  Not found   \n",
            "33326                  HSBC Online Banking                  Not found   \n",
            "33327                Halifax Security Team                  Not found   \n",
            "33328                            HSBC Bank                  Not found   \n",
            "33329                            HSBC BANK                  Not found   \n",
            "\n",
            "                                                    body  phishing  persuation  \n",
            "0      Bill:  Thanks for the info.   I also spoke wit...         0           1  \n",
            "1      Aimee,\\nPlease check meter #1591 Lamay gas lif...         0           1  \n",
            "2      GCCA Crawfish and rip-off raffle & over-priced...         0           1  \n",
            "3       <<Keoni.zip>> Chris, per your request here ar...         0           0  \n",
            "4      I'm trying to change the Receipt Meter on deal...         0           1  \n",
            "...                                                  ...       ...         ...  \n",
            "33325  Dear Valued HSBC CustomerWe recently have dete...         1           0  \n",
            "33326  __  Dear HSBC Customer ,: We have noticed some...         1           0  \n",
            "33327  Dear Hbos Customer,For your security, we have ...         1           0  \n",
            "33328  ACCOUNT LOCKED ! Dear HSBC Member,  Due to the...         1           0  \n",
            "33329  New Page 1     During our regular update and v...         1           0  \n",
            "\n",
            "[33330 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "large_dataset_file = \"../Data_Collection/final_data.csv\"\n",
        "large_dataset = pd.read_csv(large_dataset_file, encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Preprocess the large dataset\n",
        "X_large_text_features = large_dataset[[\"subject\", \"sender\", \"sender_email\", \"body\", \"phishing\"]]\n",
        "X_large = X_large_text_features.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "X_large_sequences = tokenizer.texts_to_sequences(X_large)\n",
        "X_large_padded = pad_sequences(X_large_sequences, maxlen=max_sequence_length, padding=\"post\")\n",
        "\n",
        "# Predict on the large dataset\n",
        "threshold = 0.5\n",
        "predictions = model.predict(X_large_padded)\n",
        "predicted_labels = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Add the predicted labels to the large dataset DataFrame\n",
        "large_dataset[\"persuation\"] = predicted_labels\n",
        "\n",
        "# Save the labeled dataset to a new Excel file\n",
        "labeled_dataset_file = \"labeled_dataset.xlsx\"\n",
        "large_dataset.to_excel(labeled_dataset_file, index=False)\n",
        "\n",
        "# Display the labeled dataset\n",
        "print(large_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLn7zcwu0el-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
